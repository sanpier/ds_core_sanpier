import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pingouin as pg
import seaborn as sns
import shap
import string
import time
from azureml.core import Run
from catboost import CatBoostRegressor
from imblearn.over_sampling import SMOTE
from lightgbm import LGBMRegressor
from pathos.helpers import cpu_count
from pathos.pools import ProcessPool
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
from sklearn.model_selection import cross_val_predict, train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, BaggingRegressor, StackingRegressor, VotingRegressor


class Regressor:
        
    dict_regressors = {
        "LR": LinearRegression(),
        "Ridge": Ridge(random_state=42),
        "Lasso": Lasso(random_state=42),
        "KNC": KNeighborsRegressor(),
        "SVR": SVR(), # kernel == 'poly' | 'linear' | 'sigmoid'
        "GBR": GradientBoostingRegressor(random_state=42),
        "LGBMR": LGBMRegressor(random_state=42),
        "DTR": DecisionTreeRegressor(random_state=42),
        "RFR": RandomForestRegressor(random_state=42),
        "Bagging": BaggingRegressor(random_state=42),
        "Extra": ExtraTreesRegressor(random_state=42),
        "CatBoost": CatBoostRegressor(silent=True, random_state=42)
    }

    def __init__(self, data, keep_cols, target, online=False, verbose=True):
        """ construction of Regressor class 
        """
        self.data = data[keep_cols + sorted(list(set(data.columns.tolist()) - set(keep_cols + [target]))) + [target]]
        self.target = target
        self.keep_cols = keep_cols
        if verbose:
            print("Regressor initialized")
        self.split_x_and_y(verbose)
        self.online_run = online
        if self.online_run:
            self.run = Run.get_context()

    ### TRAIN / TEST SPLIT ###
    def split_x_and_y(self, verbose=True):
        """ split target from the data 
        """
        self.features = sorted(list(set(self.data.columns.tolist()) - set(self.keep_cols + [self.target])))
        if verbose:
            print("Training will be done using the following features:\n", self.features)
        self.X = self.data[self.features].copy()
        self.y = self.data[self.target].copy()
        if verbose:
            print("Data is split into X and y:\n",
                  "\tX:", self.X.shape, "\n",
                  "\ty:", self.y.shape)

    def generate_train_test(self):
        """ create train test sets for modeling
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.3, random_state=42, shuffle=True)
        print("Train data size:", self.X_train.shape)
        print("Test data size:", self.X_test.shape)

    ### OVERSAMPLING / DATA AUGMENTATION ###
    def oversampling(self, k):
        """ oversampling method for imbalanced data
        """
        if hasattr(self, 'X_train'): 
            over = SMOTE(k_neighbors=k, random_state=42)
            self.X_train, self.y_train = over.fit_resample(self.X_train, self.y_train)
            self.X = self.X_train.copy()
            self.y = self.y_train.copy()
            print("After oversampling:\n",
                  "\tX_train:", self.X_train.shape, "\n",
                  "\ty_train:", self.y_train.shape, "\n")            
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")

    def randomized_smoothing(self, noise, extend, rounding_cols):
        """ augment training data with synthetic data generated by
            realistic gaussian noise to make modeling more robust
        """
        if hasattr(self, 'X_train'):             
            # syntetic data
            df_train = pd.concat([self.X_train, self.y_train], axis=1)
            df_train_syntetic = pd.concat([df_train]*extend)
            df_train_syntetic = np.random.normal(df_train_syntetic, df_train.std()*noise, size = df_train_syntetic.shape)
            df_train_syntetic = pd.DataFrame(data=df_train_syntetic, columns=df_train.columns)

            # postprocessing
            non_zero_cols = ((df_train < 0).any() == False).index.tolist()
            for i in non_zero_cols:
                df_train_syntetic.loc[df_train_syntetic[i] < 0, i] = 0
                
            binary_cols = df_train.columns[df_train.isin([0,1]).all()].tolist()
            for i in binary_cols:
                df_train_syntetic[i] = df_train_syntetic[i].round()
                df_train_syntetic.loc[df_train_syntetic[i] > 1, i] = 1

            for i in rounding_cols:
                df_train_syntetic[i] = df_train_syntetic[i].round()

            # concat syntetic data into original data
            df_train = pd.concat([df_train, df_train_syntetic]).reset_index(drop=True)
            self.X_train = df_train[self.features].copy()
            self.y_train = df_train[self.target].copy()
            self.X = self.X_train.copy()
            self.y = self.y_train.copy()

            print("After randomized smoothing:\n",
                  "\tTrain data size:", self.X_train.shape, "\n",
                  "\tTrain target distribution:\n", self.y_train.shape, "\n")   
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")

    ### SCORE MODELS ###
    def experiment_models(self, cv=5, transform="", ref_col=None, in_test=False):
        """ check model performances with parallel computing
        """
        cores = cpu_count()
        pool = ProcessPool(cores)
        model_names = list(self.dict_regressors.keys())
        models = list(self.dict_regressors.values())
        # experiment bunch of regression models
        print(f"Running models parallel with {cores} cores:", model_names)
        n_models = len(models)
        try:
            if in_test == True:
                scores_data = pool.amap(self.cv_score_model, models, model_names, [transform] * n_models, [ref_col] * n_models)
            else:
                scores_data = pool.amap(self.cv_score_model, models, model_names, [cv] * n_models, [transform] * n_models, [ref_col] * n_models)
            while not scores_data.ready():
                time.sleep(5); print(".", end=' ')
            scores_data = scores_data.get()
        except Exception as e:
            print(f"\nCouldn't run parallel because of the following exception:", e)
            scores_data = []
            for m_name, model in self.dict_regressors.items():
                if in_test == True:
                    scores_data.append(self.cv_score_model(model, m_name, transform, ref_col))
                else:
                    scores_data.append(self.cv_score_model(model, m_name, cv, transform, ref_col))      
        df_scores = pd.DataFrame(scores_data)     
        # sort score dataframe by R2
        df_scores.sort_values('R2', ascending=False, inplace=True)
        # best models => base models for stacking
        self.base_models = [(df_scores.iloc[0].model, self.dict_regressors[df_scores.iloc[0].model]),
                            (df_scores.iloc[1].model, self.dict_regressors[df_scores.iloc[1].model]),
                            (df_scores.iloc[2].model, self.dict_regressors[df_scores.iloc[2].model])]
        # set best model
        self.best_model = self.base_models[0]
        return df_scores

    def cv_score_model(self, model=None, model_name="", cv=5, transform="", ref_col=None):
        """ do a cross validation scoring with given model if no 
            model is given then a logistic regression will be tried
        """
        if model is None:
            if hasattr(self, 'model'): 
                model = self.model
            else:
                raise AssertionError("Please pass over a model to proceed!")
        if model_name == "":
            model_name = extract_model_name(model)
        elif model == "lr":
            model = LinearRegression()
        elif model == "best":
            model = self.best_model
        elif model == "stack":
            model = self.stacking_model()
        elif model == "vote":
            model = self.voting_model()
        self.pred_test = cross_val_predict(model, self.X, self.y, cv=cv) 
        self.model = model
        y = self.y.copy()
        pred_test = self.pred_test.copy()
        if transform == "ratio":            
            pred_test = self.X[ref_col] / pred_test
            y = self.X[ref_col] / y
        elif transform == "log": 
            pred_test = np.exp(pred_test)
            y = np.exp(y)           
        scores = regression_metrics(y, pred_test, model_name)
        return scores    

    def score_in_test(self, model=None, model_name="", transform="", ref_col=None):
        """ score the given regression model on the generated test data
        """
        if hasattr(self, 'X_train'):
            if model is None:
                if hasattr(self, 'model'): 
                    model = self.model
                else:
                    raise AssertionError("Please pass over a model to proceed!")
            if model_name == "":
                model_name = extract_model_name(model)
            elif model == "lr":
                model = LinearRegression()
            elif model == "best":
                model = self.best_model
            elif model == "stack":
                model = self.stacking_model()
            elif model == "vote":
                model = self.voting_model()
            model.fit(self.X_train, self.y_train)
            self.pred_test = model.predict(self.X_test)
            self.model = model
            y_test = self.y_test.copy()
            pred_test = self.pred_test.copy()
            if transform == "ratio":          
                pred_test = self.X_test[ref_col] / pred_test
                y_test = self.X_test[ref_col] / y_test
            elif transform == "log": 
                pred_test = np.exp(pred_test)
                y_test = np.exp(y_test) 
            scores = regression_metrics(y_test, pred_test, model_name)
            return scores             
        else:
            raise AssertionError("Please first generate train & test datasets out of given data!")
       
    ### ENSEMBL MODELS ### 
    def define_base_models(self, base_list):
        """ give a list of model abbreviations to be used
            in stacking
        """
        self.base_models = []
        for name in base_list:
            self.base_models.append((name, self.dict_regressors[name]))
        print("Base models are defined!")

    def stacking_model(self):
        """ create a stacking model using best 3 base models
            that are defined after experimenting all models
        """
        if hasattr(self, 'base_models'): 
            meta_model = LinearRegression()
            final_model = StackingRegressor(estimators=self.base_models, final_estimator=meta_model, cv=5)
            return final_model
        else:
            raise AssertionError("Please first experiment models to set top 3 base models!")

    def voting_model(self):
        """ create a voting model using best 3 base models
            that are defined after experimenting all models
        """
        if hasattr(self, 'base_models'): 
            final_model = VotingRegressor(estimators=self.base_models)
            return final_model
        else:
            raise AssertionError("Please first experiment models to set top 3 base models!")

    ### TRAIN GIVEN MODEL & PREDICT GIVEN TEST ###
    def train_model(self, model=None):
        """ train the given regression model on whole data
        """   
        if model is None:
            if hasattr(self, 'model'): 
                model = self.model
            else:
                raise AssertionError("Please pass over a model to proceed!")
        elif model == "lr":
            model = LinearRegression()
        elif model == "best":
            model = self.best_model
        elif model == "stack":
            model = self.stacking_model()
        elif model == "vote":
            model = self.voting_model()
        model.fit(self.X, self.y)
        print("Model is fit on whole data!")
        self.trained_model = model

    def predict_test(self):
        """ train the given regression model on whole data
        """   
        if hasattr(self, 'trained_model'): 
            return self.trained_model.predict(self.X)
        else:
            raise AssertionError("Please first train a model then predict on the test data!")

    ### EXPLAIN MODEL ###
    def explain_model_with_shap(self, model=None):
        """ show SHAP values to explain the output of the regression model
        """
        if model is None:
            if hasattr(self, 'model'): 
                model = self.model
            else:
                raise AssertionError("Please pass over a model to proceed!")
        # fit the model
        self.train_model(model)
        # set important features
        importances = model.feature_importances_
        sorted_idx = importances.argsort()[(-1*self.X.shape[1]):]
        important_features = self.X.columns[sorted_idx].tolist() 
        # get shap summary plot
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(self.X)
        shap.summary_plot(shap_values, self.X, feature_names = important_features)

    def residual_difference(self, lower_threshold=0.4, higher_threshold=2.5, res=0.05, name='', transform="", ref_col=None):
        """ show histogram distribution of the ratio between
            selected two continuous columns in the dataframe
            that wanted to be same in ideal case 
        """
        if hasattr(self, 'pred_test'): 
            df_ratio = self.data.rename(columns={self.target: "actual"})
            df_ratio["prediction"] = self.pred_test
            if transform == "ratio":          
                df_ratio["prediction"] = df_ratio[ref_col] / self.pred_test
                df_ratio["actual"] = df_ratio[ref_col] / self.y
            elif transform == "log": 
                df_ratio["prediction"] = np.exp(self.pred_test)
                df_ratio["actual"] = np.exp(self.y) 
            df_ratio["error"] = df_ratio["prediction"] - df_ratio["actual"]
            df_ratio["percentage_error"] = df_ratio["error"]/df_ratio["actual"] 
            df_ratio["abs_percentage_error"] = round(100*(np.absolute(df_ratio["percentage_error"])), 1) 
            df_ratio["ratio"] = df_ratio["prediction"] / df_ratio["actual"]
            df_ratio_ = df_ratio[(df_ratio["ratio"] > lower_threshold) & (df_ratio["ratio"] < higher_threshold)]    

            # number of observations per percentage error
            percentage_errors = [5, 10, 15, 25, 50]
            print("Number of observations per percentage error:")
            [print(f'\t<={val}%:', df_ratio[df_ratio["abs_percentage_error"] <= val].shape[0]) for val in percentage_errors]
            df_ratio["evaluation"] = df_ratio["abs_percentage_error"].apply(evaluate)

            fig, axes = plt.subplots(3, 2, figsize=(18, 21))
            # 1. histogram distribution
            plt.figure(figsize=(24, 27))
            sns.histplot(data=df_ratio.iloc[df_ratio_.index], x="ratio", hue="evaluation", binwidth=res,
                         palette={"5%":  "#205072", 
                                  "10%": "#33709c", 
                                  "15%": "#329D9C",
                                  "25%": "#56C596",
                                  "50%": "#7BE495",
                                  "off": "#CFF4D2"}, hue_order = ["5%", "10%", "15%", "25%", "50%", "off"], ax=axes[0][0])
            axes[0][0].set(xlabel='Ratio', ylabel='Count', title='Histogram Distribution of Ratio')
            # 2. plot regression plot in colors
            palette={"5%":  "#1c3e5c",  
                     "10%": "#224b6e",
                     "15%": "#306896", 
                     "25%": "#41739c",
                     "50%": "#679bc7",
                     "off": "#caddee"}
            sns.regplot(x=df_ratio['prediction'], y=df_ratio['actual'], order=2, ax=axes[0][1])
            sns.scatterplot(data=df_ratio, x="prediction", y="actual", hue="evaluation", palette=palette, hue_order = ["5%", "10%", "15%", "25%", "50%", "off"], ax=axes[0][1])
            axes[0][1].set(xlabel='Predicted', ylabel='Target', title='Regression Line')
            # 3. plot predictions vs residuals 
            sns.regplot(x=df_ratio["prediction"], y=df_ratio["error"], color='b', order=2, ax=axes[1][0])
            axes[1][0].set(xlabel='Predicted', ylabel='Residual', title='Predictions vs Residuals')
            # 4. plot predictions vs percentage error
            sns.regplot(x=df_ratio["prediction"], y=df_ratio["percentage_error"], color='b', order=2, ax=axes[1][1])
            axes[1][1].set(xlabel='Predicted', ylabel='Ratio Residual', title='Predictions vs Ratio Residuals')
            # 5. plot qq plot of residuals
            pg.qqplot(df_ratio["error"], dist='norm', ax=axes[2][0]) # figsize=(12,9)
            axes[2][0].set(title='QQ-Plot of Residuals')
            # 6. plot qq plot of percentage error
            pg.qqplot(df_ratio["percentage_error"], dist='norm', ax=axes[2][1])
            axes[2][1].set(title='QQ-Plot of Ratio Residuals')
            
            fig.tight_layout()
            plt.show()
            # save figure if on cloud
            if self.online_run:
                filename=f'./outputs/regplot_{name}.png'
                plt.savefig(filename, dpi=600)
                plt.close()

            return df_ratio
        else:
            raise AssertionError("Please first have predictions to check the distribution and scoring metrics!")

    ### QUANTILE REGRESSION ###
    def quantile_regression(self, postprocessing, threshold_interval, quantiles=[0.5, 0.01, 0.05, 0.075, 0.1, 0.125, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]): 
        """ provides quantile regression predictions for all the given list
            of quantiles
        """
        df = pd.DataFrame({"ground-truth": self.y})
        for q in quantiles:
            model = LGBMRegressor(objective = 'quantile', alpha = q, random_state=42)            
            if q == 0.5:
                scores = self.cv_score_model(model)
                best_model = model
                df["best_prediction"] = self.pred_test
                df["abs_percentage_error"] = round(100*(np.absolute(self.pred_test - self.y)/self.y), 1)
                df["ratio"] = df["best_prediction"] / df["ground-truth"]
                # report best predictions
                print("Best quantile predictions:")
                [print('\t', key,':', val) for key, val in scores.items()]
                if self.online_run:
                    [self.run.log(f"{key}:", val) for key, val in scores.items()]  
            else:
                _ = self.cv_score_model(model)
                df[f"{q}_pred"] = self.pred_test
                model = LGBMRegressor(objective = 'quantile', alpha = 1-q, random_state=42)            
                _ = self.cv_score_model(model)
                df[f"{1-q}_pred"] = self.pred_test 
                df[f"{q}-{1-q}_interval"] = df[f"{1-q}_pred"] - df[f"{q}_pred"]
                
                # postprocessing of quantile regression results
                if postprocessing:
                    df = self.postprocessing_quantile_regression(df, f"{q}_pred", f"{1-q}_pred", f"{q}-{1-q}_interval", threshold_interval)

                # evaluate result
                df[f"{q}-{1-q}_evaluation"] = np.where(((df[f"{q}_pred"] <= df["ground-truth"]) & (df[f"{1-q}_pred"] >= df["ground-truth"])), "Good", "Bad")
                print("\nQuantiles:", q, 1-q)
                print(round(df[f"{q}-{1-q}_interval"].mean(), 2), "is the average of quantile range interval")
                df_evaluation = df.groupby(f"{q}-{1-q}_evaluation").agg({'abs_percentage_error': 'mean', 'ground-truth': 'count'})
                df_evaluation["percentage"] = df_evaluation["ground-truth"].apply(lambda x: "{0:.1%}".format(x/df_evaluation["ground-truth"].sum())) 
                print(df_evaluation)
        # set best 
        self.model = best_model   
        self.pred_test = df["best_prediction"] 
        return df
        
    def quantile_regression_low_high_interval(self, threshold_interval, quantile): 
        """ provides quantile regression predictions for all the given list
            of quantiles
        """
        df = pd.DataFrame({"ground-truth": self.y})

        # for quantile = quantile
        model = LGBMRegressor(objective = 'quantile', alpha = quantile, random_state=42)            
        _ = self.cv_score_model(model)
        df["lowest_prediction"] = self.pred_test.round()
        model = LGBMRegressor(objective = 'quantile', alpha = 1-quantile, random_state=42)            
        _ = self.cv_score_model(model)
        df["highest_prediction"] = self.pred_test.round() 
        df["interval"] = df["highest_prediction"] - df["lowest_prediction"]
  
        # for quantile = 0.5
        model = LGBMRegressor(objective = 'quantile', alpha = 0.5, random_state=42)            
        scores = self.cv_score_model(model)
        self.model = model   
        df["best_prediction"] = self.pred_test.round()
        df["abs_percentage_error"] = round(100*(np.absolute(self.pred_test - self.y)/self.y), 1)
        df["ratio"] = df["best_prediction"] / df["ground-truth"]
        
        # postprocessing of quantile regression results
        df = self.postprocessing_quantile_regression(df, "lowest_prediction", "highest_prediction", "interval", threshold_interval)

        # report best predictions
        print("Best quantile predictions:")
        [print('\t', key,':', val) for key, val in scores.items()]
        if self.online_run:
            [self.run.log(f"{key}:", val) for key, val in scores.items()]  

        # evaluate result
        df["evaluation"] = np.where(((df["lowest_prediction"] <= df["ground-truth"]) & (df["highest_prediction"] >= df["ground-truth"])), "Good", "Bad")
        print("\nQuantiles:", quantile, 1-quantile)
        print(round(df["interval"].mean(), 2), "is the average of quantile range interval")
        df_evaluation = df.groupby("evaluation").agg({'abs_percentage_error': 'mean', 'ground-truth': 'count'})
        df_evaluation["percentage"] = df_evaluation["ground-truth"].apply(lambda x: "{0:.1%}".format(x/df_evaluation["ground-truth"].sum())) 
        print(df_evaluation)

        # set best   
        self.pred_test = df["best_prediction"] 
        return df

    def postprocessing_quantile_regression(self, results, lowest_col, highest_col, interval_col, threshold_interval): 
        """ quantile regression postprocessing """       
        # conditions to change interval predictions 
        condition1 = (results[lowest_col] > results[highest_col])
        condition2 = (results[lowest_col] > results["best_prediction"]) | (results["best_prediction"] > results[highest_col]) 
        condition3 = results[interval_col] <= threshold_interval
        cond = condition1 | condition2 | condition3
        results.loc[cond, lowest_col] = results.loc[cond, "best_prediction"] - threshold_interval/2
        results.loc[cond, highest_col] = results.loc[cond, "best_prediction"] + threshold_interval/2
        results[interval_col] = results[highest_col] - results[lowest_col]
        return results
                    

### AUXILIARY FUNCTIONS ###    
def evaluate(x):
    """ evaluation intervals of the residual ratio between
        two continious features
    """
    if (x >= 50):
        value = "off"
    if (x <= 50):
        value = "50%"
    if (x <= 25):
        value = "25%"
    if (x <= 15):
        value = "15%"
    if (x <= 10):
        value = "10%"
    if (x <= 5):
        value = "5%"
    return value

def regression_metrics(y_test, pred_test, model_name=""):
    """ show metrics for selected two continuous columns to be
        compared with each other 
    """   
    return {
        'model' : model_name,
        'R2': round(r2_score(y_test, pred_test), 3),
        'MAE': round(mean_absolute_error(y_test, pred_test), 3),
        'MAPE': round(mean_absolute_percentage_error(y_test, pred_test), 3),
        'RMSE': round(mean_squared_error(y_test, pred_test, squared=False), 3),
        'sample_size': len(y_test),    
    }

def extract_model_name(model):
    model_name = str(type(model)).split(".")[-1]
    return "".join([char for char in model_name if char not in string.punctuation])